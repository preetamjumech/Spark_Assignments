{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 5_SparkSQL SQLContext HiveContext_Preetam Saha_C22018_25.05.2022",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cigqrh_QpSPu"
      },
      "source": [
        "# SPARK SQL\n",
        "If you wish to run HIVE natively under Hadoop please see this notebook [Hadoop and Hive](https://github.com/prithwis/KKolab/blob/main/KK_B2_Hadoop_and_Hive.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQxaKP3-ugUs"
      },
      "source": [
        "# Install Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manual Install is required because we need to use an Spark 3.0.3 to avoid errors"
      ],
      "metadata": {
        "id": "rtng1x3mBon1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aG7A8TjulYO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "7edb7ab3-d1e6-44ef-afe8-add840938d02"
      },
      "source": [
        "!apt-get update > /dev/null\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-2.2.2/spark-2.2.2-bin-hadoop2.7.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop3.2.tgz\n",
        "#!wget -q http://apache.osuosl.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz     --- this gives errors\n",
        "!wget -q https://apache.osuosl.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop3.2.tgz     # Using Older Version \n",
        "\n",
        "#\n",
        "# if the current version of Spark is not used, there may be errors\n",
        "# check here for current versions http://apache.osuosl.org/spark\n",
        "#\n",
        "#!tar xf spark-2.4.0-bin-hadoop2.7.tgz\n",
        "#!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "#!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "#!tar xf spark-3.0.1-bin-hadoop3.2.tgz\n",
        "#!tar xf spark-3.1.2-bin-hadoop3.2.tgz -- this gives errors\n",
        "!tar xf spark-3.0.3-bin-hadoop3.2.tgz\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop3.2\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop3.2\"\n",
        "\n",
        "!pip install -q findspark  # findspark is no more required\n",
        "#!pip install -q pyspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "# -----------------------------------------------\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "sc"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://58e7ed818954:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2022 Approach Does not work since it installs the current version of Spark"
      ],
      "metadata": {
        "id": "3P0Z_WDSBsQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip3 -q install pyspark\n",
        "#from pyspark.sql import SparkSession\n",
        "#spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "#sc = spark.sparkContext\n",
        "#sc"
      ],
      "metadata": {
        "id": "36uWkLtcBx4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAyLqUDFyjjQ"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOLe2EvPyuya"
      },
      "source": [
        "#we remove the CRLF character from the end of the row if it exists\n",
        "!sed 's/\\r//' //content/Employee.csv > employee.csv\n",
        "!sed 's/\\r//' //content/Department.csv > department.csv\n",
        "#!sed 's/\\r//' /content/eCommerce_02PC_2021.csv > datafile.csv\n",
        "#sed -i -e \"1d\" datafile.csv               # remove the first line containing headers from the file"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqcZoCSe9QjY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f5462f1-eba0-4a2d-caff-eea43342077f"
      },
      "source": [
        "data_file = 'employee.csv'\n",
        "raw_data = sc.textFile(data_file)\n",
        "print (\"Data Size\", raw_data.count())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Size 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_file1 = 'department.csv'\n",
        "raw_data1 = sc.textFile(data_file1)\n",
        "print (\"Data Size\", raw_data1.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySYIfWTTybgj",
        "outputId": "8cf1a00a-bf34-416d-e24e-107ed8b1ac1e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Size 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYyko_-I1DnL"
      },
      "source": [
        "#Spark Data Frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjnrF4hrzwbj"
      },
      "source": [
        "employee_df = spark.read.csv(data_file,inferSchema=True, header=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "department_df = spark.read.csv(data_file1,inferSchema=True, header=True)"
      ],
      "metadata": {
        "id": "TRO4w8vny4S-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a8pJFwk0F3p",
        "outputId": "5e277b7f-a4f8-4922-94e2-8cbc5678460c"
      },
      "source": [
        "employee_df.printSchema()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- EmpID: integer (nullable = true)\n",
            " |-- LastName: string (nullable = true)\n",
            " |-- FirstName: string (nullable = true)\n",
            " |-- JobDesc: string (nullable = true)\n",
            " |-- JoinDate: string (nullable = true)\n",
            " |-- Salary: integer (nullable = true)\n",
            " |-- Comm: double (nullable = true)\n",
            " |-- DeptID: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "department_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Nn_oHSXzAQA",
        "outputId": "a424a06c-abfa-41b6-d532-7c2b944b9989"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- DeptID: integer (nullable = true)\n",
            " |-- DeptName: string (nullable = true)\n",
            " |-- ManagerID: integer (nullable = true)\n",
            " |-- Location: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OH_nAI751nuQ",
        "outputId": "ac77d369-8df6-4dbe-8c46-1ba3638c472a"
      },
      "source": [
        "employee_df.select('LastName','FirstName','EmpID','DeptID').groupby(employee_df.DeptID).count().sort('count', ascending = False).show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|DeptID|count|\n",
            "+------+-----+\n",
            "|    10|    3|\n",
            "|    40|    3|\n",
            "|    20|    3|\n",
            "|    30|    3|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "department_df.select('DeptID','DeptName','Location').groupby(department_df.Location).count().sort('count', ascending = False).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BE6l24PXzGhB",
        "outputId": "e039bb1e-752a-4a40-f0a4-68f527b370f5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----+\n",
            "|Location|count|\n",
            "+--------+-----+\n",
            "|Calcutta|    3|\n",
            "|  Bombay|    1|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "employee_df.select('LastName','FirstName','EmpID','DeptID').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "900CFDbhtF0I",
        "outputId": "a1c030f0-61df-454d-99b6-38d8de96dbdf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------+------+------+\n",
            "|   LastName|FirstName| EmpID|DeptID|\n",
            "+-----------+---------+------+------+\n",
            "|    Bacchan|  Amitabh|742866|    10|\n",
            "|  Mukherjee|     Rani|349870|    40|\n",
            "|    Dikshit|  Madhuri|865477|    20|\n",
            "|       Khan| Shahrukh|239456|    20|\n",
            "|     Sehwag| Virender|897889|    20|\n",
            "|      Dhoni| Mahender|123980|    40|\n",
            "|     Dravid|    Rahul|822134|    30|\n",
            "|     Dalmia| Jagmohan|997445|    30|\n",
            "|    Ganguly|   Sourav|989007|    40|\n",
            "|    Ganesan|    Rekha|299034|    10|\n",
            "|Karthikeyan|  Narayan|546223|    10|\n",
            "|      Mirza|    Sania|223112|    30|\n",
            "+-----------+---------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "department_df.select('DeptID','DeptName','Location').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmJN1t2pz4mR",
        "outputId": "bae104ea-2a1b-445a-a8ab-1691b5b3c706"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+--------+\n",
            "|DeptID|  DeptName|Location|\n",
            "+------+----------+--------+\n",
            "|    10| Corporate|Calcutta|\n",
            "|    20|     Sales|Calcutta|\n",
            "|    30|  Accounts|Calcutta|\n",
            "|    40|Production|  Bombay|\n",
            "+------+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKzL9g6-3yX9"
      },
      "source": [
        "#Spark SQL\n",
        "What is the difference between SQLContext and HiveContext? See [here](https://intellipaat.com/community/7599/what-is-the-difference-between-apache-spark-sqlcontext-vs-hivecontext#:~:text=HiveContext%20is%20a%20super%20set,read%20data%20from%20Hive%20tables.&text=The%20more%20basic%20SQLContext%20provides,does%20not%20depend%20on%20Hive.), or [here](https://stackoverflow.com/questions/33666545/what-is-the-difference-between-apache-spark-sqlcontext-vs-hivecontext)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SFUz9L-gfaC"
      },
      "source": [
        "commas getting removed here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXMc_yIV89IE"
      },
      "source": [
        "#eCommerce2_df = sqlContext.createDataFrame(row_data)\n",
        "#eCommerce2_df.registerTempTable(\"eCommerce\")\n",
        "# spark 2 onwards ...\n",
        "\n",
        "employee_df.createOrReplaceTempView(\"employee\")\n",
        "department_df.createOrReplaceTempView(\"department\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q39dzGqGdTb",
        "outputId": "725dcd60-a6c0-42e1-b15e-49dd3e4e8b86"
      },
      "source": [
        "#Countries = sqlContext.sql(\"SELECT * FROM eCommerce limit 20\")\n",
        "employees = spark.sql(\"SELECT * FROM employee limit 20\")\n",
        "employees.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+---------+----------+----------+------+----+------+\n",
            "| EmpID|   LastName|FirstName|   JobDesc|  JoinDate|Salary|Comm|DeptID|\n",
            "+------+-----------+---------+----------+----------+------+----+------+\n",
            "|742866|    Bacchan|  Amitabh| Executive|2003-03-10| 50000| 0.1|    10|\n",
            "|349870|  Mukherjee|     Rani|   Manager|2005-05-04| 25000|0.06|    40|\n",
            "|865477|    Dikshit|  Madhuri|     Clerk|2002-04-04| 10000|0.02|    20|\n",
            "|239456|       Khan| Shahrukh|   Manager|2004-01-03| 30000|0.07|    20|\n",
            "|897889|     Sehwag| Virender|   Cus_Rep|2005-01-02| 15000|0.05|    20|\n",
            "|123980|      Dhoni| Mahender|     Clerk|2004-10-09|  9000|0.02|    40|\n",
            "|822134|     Dravid|    Rahul|Sr Manager|2000-06-04| 40000|0.08|    30|\n",
            "|997445|     Dalmia| Jagmohan|     Clerk|2001-07-01| 12000|0.02|    30|\n",
            "|989007|    Ganguly|   Sourav|   Cus_Rep|2002-01-01| 20000|0.03|    40|\n",
            "|299034|    Ganesan|    Rekha|  Director|2002-10-10| 60000|0.11|    10|\n",
            "|546223|Karthikeyan|  Narayan| Secretary|2005-12-04| 40000|0.09|    10|\n",
            "|223112|      Mirza|    Sania|   Cus_Rep|2001-11-19| 25000|0.04|    30|\n",
            "+------+-----------+---------+----------+----------+------+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "department = spark.sql(\"SELECT * FROM department limit 20\")\n",
        "department.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Tx1OVbb0Lip",
        "outputId": "8284c016-7a69-47a8-e995-23fe7c19ea14"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+---------+--------+\n",
            "|DeptID|  DeptName|ManagerID|Location|\n",
            "+------+----------+---------+--------+\n",
            "|    10| Corporate|   299034|Calcutta|\n",
            "|    20|     Sales|   239456|Calcutta|\n",
            "|    30|  Accounts|   822134|Calcutta|\n",
            "|    40|Production|   349870|  Bombay|\n",
            "+------+----------+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYcC-W_AHGPq",
        "outputId": "5798f2ae-7374-4de6-a8f2-ff74d2b68ab7"
      },
      "source": [
        "#Countries = sqlContext.sql(\"SELECT sum(Quantity), sum(UnitPrice), Country from eCommerce group by Country order by sum(Quantity) desc\")\n",
        "employees = spark.sql(\"SELECT DeptID,sum(Salary), avg(Comm)  from employee group by DeptID  order by sum(Salary) desc\")\n",
        "employees.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+-------------------+\n",
            "|DeptID|sum(Salary)|          avg(Comm)|\n",
            "+------+-----------+-------------------+\n",
            "|    10|     150000|0.10000000000000002|\n",
            "|    30|      77000|0.04666666666666667|\n",
            "|    20|      55000|0.04666666666666667|\n",
            "|    40|      54000|0.03666666666666667|\n",
            "+------+-----------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "employees = spark.sql(\"select * from employee join department where employee.DeptID = department.DeptID\")\n",
        "employees.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UhSotTlxU7e",
        "outputId": "772ebe58-8e5c-40d6-8b54-54fe47000224"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+---------+----------+----------+------+----+------+------+----------+---------+--------+\n",
            "| EmpID|   LastName|FirstName|   JobDesc|  JoinDate|Salary|Comm|DeptID|DeptID|  DeptName|ManagerID|Location|\n",
            "+------+-----------+---------+----------+----------+------+----+------+------+----------+---------+--------+\n",
            "|742866|    Bacchan|  Amitabh| Executive|2003-03-10| 50000| 0.1|    10|    10| Corporate|   299034|Calcutta|\n",
            "|349870|  Mukherjee|     Rani|   Manager|2005-05-04| 25000|0.06|    40|    40|Production|   349870|  Bombay|\n",
            "|865477|    Dikshit|  Madhuri|     Clerk|2002-04-04| 10000|0.02|    20|    20|     Sales|   239456|Calcutta|\n",
            "|239456|       Khan| Shahrukh|   Manager|2004-01-03| 30000|0.07|    20|    20|     Sales|   239456|Calcutta|\n",
            "|897889|     Sehwag| Virender|   Cus_Rep|2005-01-02| 15000|0.05|    20|    20|     Sales|   239456|Calcutta|\n",
            "|123980|      Dhoni| Mahender|     Clerk|2004-10-09|  9000|0.02|    40|    40|Production|   349870|  Bombay|\n",
            "|822134|     Dravid|    Rahul|Sr Manager|2000-06-04| 40000|0.08|    30|    30|  Accounts|   822134|Calcutta|\n",
            "|997445|     Dalmia| Jagmohan|     Clerk|2001-07-01| 12000|0.02|    30|    30|  Accounts|   822134|Calcutta|\n",
            "|989007|    Ganguly|   Sourav|   Cus_Rep|2002-01-01| 20000|0.03|    40|    40|Production|   349870|  Bombay|\n",
            "|299034|    Ganesan|    Rekha|  Director|2002-10-10| 60000|0.11|    10|    10| Corporate|   299034|Calcutta|\n",
            "|546223|Karthikeyan|  Narayan| Secretary|2005-12-04| 40000|0.09|    10|    10| Corporate|   299034|Calcutta|\n",
            "|223112|      Mirza|    Sania|   Cus_Rep|2001-11-19| 25000|0.04|    30|    30|  Accounts|   822134|Calcutta|\n",
            "+------+-----------+---------+----------+----------+------+----+------+------+----------+---------+--------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}